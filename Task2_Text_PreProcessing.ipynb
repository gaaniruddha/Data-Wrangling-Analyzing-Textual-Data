{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name: Gayatri Aniruddha \n",
    "#### Student ID: 30945305\n",
    "\n",
    "Date: 29/08/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 2.7.11 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* re (for regular expression, included in Anaconda Python 2.7) \n",
    "* langid (to classift the language of a tweet)\n",
    "* nltk 3.2.2 (Natural Language Toolkit, included in Anaconda Python 3.6)\n",
    "* nltk.collocations (for finding bigrams)\n",
    "* nltk.tokenize (for tokenization, for both single words and multi-word expressions)\n",
    "* nltk.stem (for porter stemmer)\n",
    "* itertools (for iteration methods)\n",
    "* nltk.collocations (for unigrams and bigrams)\n",
    "* sklearn.feature_extraction.text (for CounterVectorizationn, in order to generate sparse matrix representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction \n",
    "* We have the following dataset : \n",
    "    * We have an xlsx file which has around 80 sheets.\n",
    "    * Each sheet has information about COVID-19 tweets.\n",
    "    * Each sheet has around 2000 tweets.\n",
    "    * We have the following information about each tweet :\n",
    "        * tweet_id\n",
    "        * text\n",
    "        * created_at \n",
    "* Tasks performed:\n",
    "    * Generation of a corpus vocabulary with the same structure as sample_vocab.txt.\n",
    "    * Calculation and presentation of the top 100 frequent unigrams for a particular day i.e sheet.\n",
    "    * Calculation and presentation of the top 100 frequent bigrams for a particular day i.e sheet.\n",
    "    * Generation of a sparse representation as per sample_countVec.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology \n",
    "* I have followed the following set of steps for data extraction and data manipultion.\n",
    "\n",
    "### 2.1 Importing Libraries\n",
    "\n",
    "* Here, I have imported the following libraries for our assignment.\n",
    "* I have even mentioned the need, usage and purpose of each of the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for regular expression \n",
    "import re\n",
    "\n",
    "# Library to work with dataframes \n",
    "import pandas as pd\n",
    "\n",
    "# Library to filter out the non engligh tweets \n",
    "import langid\n",
    "\n",
    "# Library for Natural Language Processing\n",
    "import nltk\n",
    "\n",
    "# Importing the required packages from langid package\n",
    "# These packages are imported for separating out the non english tweets \n",
    "# For identifying English tweets \n",
    "from langid.langid import LanguageIdentifier \n",
    "\n",
    "# Used as a parameter so that it comes in 0 to 1 range \n",
    "from langid.langid import model \n",
    "\n",
    "# For Unigram generation \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Multiword Expressions\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# For calculating document frequency \n",
    "from nltk.probability import *\n",
    "\n",
    "# For iteration methods\n",
    "import itertools\n",
    "# In order to join all the tokens later on in the document\n",
    "from itertools import chain\n",
    "\n",
    "# For Porter Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# For Bigram generation \n",
    "from nltk.collocations import *\n",
    "\n",
    "# For creating count vectors \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Identifier Initialisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the identifier with normal probability True \n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Excel Sheet Parsing\n",
    "\n",
    "* Here, I have loaded all the sheets into one dataframe using read_excel function\n",
    "* Here :\n",
    "    * Key : Sheet Name\n",
    "    * Value : Each sheet in the excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading our excel table into a dataframe \n",
    "# sheet_name and header are NONE : In order to merge all the sheets into one dataframe \n",
    "\n",
    "# Here we open our Excel file by creating a Pandas ExcelFile object named excel_data \n",
    "#excel_data = pd.read_excel('sample.xlsx', sheet_name = None, header= None)\n",
    "excel_data = pd.read_excel('30945305.xlsx', sheet_name = None, header= None)\n",
    "\n",
    "# Key : Sheet Names\n",
    "# Value : Each sheet in the excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Generating a list of stopwords\n",
    "\n",
    "* Here, we create a list of stopwords.\n",
    "* This list has all the stopwords given in the stopwords text file provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unigrams - Regular Expression Tokenizer \n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "# Creating a list to add all the given stop words\n",
    "stop_words_list = []\n",
    "\n",
    "# Opening the file in read mode and utf encoding \n",
    "with open('stopwords_en.txt', 'r', encoding = 'utf8') as stopwords:\n",
    "      \n",
    "    # We iterate through the stopwords text file\n",
    "    for line in stopwords:\n",
    "        \n",
    "        # Adding the words to the stop_words_list after stripping the '\\n'\n",
    "        stop_words_list.append(line.rstrip('\\n'))\n",
    "        \n",
    "# Creating a set of stopwords in order to eliminate duplicate words\n",
    "stop_words_set = set(stop_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Initilising our tokenizer\n",
    "\n",
    "* This regular expression is as per the documentation provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As per the document specification\n",
    "# Here, we are initiliazing our tokenizer \n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Data Cleaning, Analysis and Tokenization\n",
    "\n",
    "* I have performed the following **13** data cleaning actions :\n",
    "    * Removal of columns which have all Null Values.\n",
    "    * Removal of rows which have all Null Values.\n",
    "    * Generation of tokens for every tweet for every sheet.\n",
    "    * Removal of Non English Tweets.\n",
    "    * Removal of context independent stopwords.\n",
    "        * We remove the tokens whose length is less than 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Basic Data Cleaning of the Excel Sheets\n",
    "# Iterating through every df i.e sheet \n",
    "for key, df in excel_data.items():\n",
    "    \n",
    "    # Step 1 : we are dropping all columns with null values\n",
    "    df.dropna(how = \"all\", axis = 1, inplace = True)\n",
    "    \n",
    "    # Step 2 : we are dropping all rows with null values\n",
    "    df.dropna(how = \"all\", axis = 0, inplace = True)\n",
    "    \n",
    "    # Step 3 : we are Changing columns names \n",
    "    df.columns = ['text','id','created_at']\n",
    "    \n",
    "    # Step 4 : we are dropping the header\n",
    "    df.drop(df.head(1).index, inplace=True)\n",
    "    \n",
    "    # Step 5 : we are removing duplicates\n",
    "    df.drop_duplicates('id', keep = 'first', inplace=True)\n",
    "       \n",
    "    # Step 6 : Here, Converting the string to lower case\n",
    "    # To remove redundant tokens \n",
    "    df['text'] = df['text'].apply(lambda x:str(x).lower())\n",
    "    \n",
    "    # Adding column for langugage\n",
    "    df['language'] = \"\"\n",
    "    \n",
    "    # THIS STEP IS TAKING TIME \n",
    "    # Step 7 : Adding the tweet language \n",
    "    for index, row in df.iterrows():\n",
    "        row['language'] = identifier.classify(row['text'])[0]\n",
    "    \n",
    "    # Step 8 : Drop all rows with non english tweets \n",
    "    df.drop(df.loc[df['language']!='en'].index, inplace=True)\n",
    "    \n",
    "    # Step 9 : Adding column for tokens \n",
    "    df['tokens'] = \"\"\n",
    "    \n",
    "    # Step 10 : Adding a column which has tokens generated for that row \n",
    "    df['tokens'] = df['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    \n",
    "    # Step 11 : Removing the context independent stopwords from the tokens \n",
    "    df['final_tokens'] = df['tokens'].apply(lambda x: [token for token in x if token not in stop_words_list])\n",
    "    \n",
    "    # Step 12 : Removing tokens with the length less than 3 \n",
    "    df['final_tokens'] = df['final_tokens'].apply(lambda x: [token for token in x if len(token)>=3])\n",
    "         \n",
    "    # Step 13 : Dropping the old index and keeping only the new one \n",
    "    df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Removal of Context Dependent words\n",
    "\n",
    "* Here, we first create a list of all the tokens using chaining.\n",
    "* We then create a set of these unique words.\n",
    "* We then generate a document frequency.\n",
    "    * This gives us the number of times a given word appears.\n",
    "* Removal of tokens which appear in lesser than **5 documents.**\n",
    "     * Here, these are rare tokens.\n",
    "     * They have less variance.\n",
    "     * They also have lesser correlation with the topic! \n",
    "* Removing tokens which appear in more than **60 documents.**\n",
    "     * Here, these tokens give repetitive information.\n",
    "     * They have high correlation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Context Dependent words \n",
    "\n",
    "# For creating chain of tokens \n",
    "from __future__ import division\n",
    "from itertools import chain\n",
    "\n",
    "# List of lists, chaining everything \n",
    "# List of every token - tokenized all the sheets\n",
    "# And then add all the tokens to the same list \n",
    "words = list(chain.from_iterable([set(list(chain(*df['final_tokens']))) for key, df in excel_data.items()]))\n",
    "\n",
    "# Creating unique set of tokens \n",
    "vocab = set(words)\n",
    "\n",
    "# Creating document frequency \n",
    "# Word : Number of documents in which the word appears \n",
    "fd = FreqDist(words)\n",
    "#fd.most_common(5)\n",
    "\n",
    "# Removing rare tokens\n",
    "# Here : Each document represents a day \n",
    "\n",
    "# Words appearing in lesser then 5 documents i.e 5 days - 1 : for trials \n",
    "# Less Correlation!\n",
    "less_than_5_days = list(filter(lambda x:fd[x] < 1, fd))\n",
    "\n",
    "# Words appearing in greater then 60 documents i.e 60 days - 6 : for trials \n",
    "# Over information! \n",
    "greater_than_60_days = list(filter(lambda x:fd[x] > 6, fd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Porter Stemming, Unigram and Bigram generation\n",
    "\n",
    "* Here, we use Porter Stemming to further compress our data.\n",
    "* I have defined a bigram_generator function :\n",
    "    * This is as per the content taken from tutorial 5.\n",
    "    * This is to generate the top 100 bigrams.\n",
    "* Finally, I have created an unigrams and bigrams dictionary.\n",
    "    * In Unigrams, individual words are considered.\n",
    "    * In Bigrams, group of two words are considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "# Removing Context Dependent words to get the final set of tokens \n",
    "%%time\n",
    "\n",
    "# Initilising our Porter Stemmer \n",
    "stem = PorterStemmer()\n",
    "\n",
    "# Unigrams Dictionary\n",
    "uni_grams = dict()\n",
    "\n",
    "# Bigrams Dictionary\n",
    "bi_grams = dict()\n",
    "\n",
    "# Bigram Generator \n",
    "# Function to generate top 200 bigrams using PMI Measure - Taken from Tutorial 5.\n",
    "def bigrams_generator(l):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(l)\n",
    "    bigram_finder.apply_freq_filter(20)\n",
    "    #bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)\n",
    "    top_100_bigrams = bigram_finder.nbest(bigram_measures.pmi, 100) # Top-100 bigrams\n",
    "    return top_100_bigrams\n",
    "\n",
    "# Iterating through every df i.e sheet \n",
    "for key, df in excel_data.items():\n",
    "    \n",
    "    # Removing words appearing in greater then 60 sheets i.e 60 days\n",
    "    df['final_tokens_2'] = df['final_tokens'].apply(lambda x: [token for token in x if token not in greater_than_60_days])\n",
    "    \n",
    "    # Removing words appearing in lesser then 5 sheets i.e 5 days\n",
    "    df['final_tokens_2'] = df['final_tokens'].apply(lambda x: [token for token in x if token not in less_than_5_days])\n",
    "    \n",
    "    # Creating a new column for Porter Stemming \n",
    "    # Porter stemming of the final set of tokens \n",
    "    df['stem_tokens'] = df['final_tokens_2'].apply(lambda x:[stem.stem(token) for token in x])\n",
    "    \n",
    "    # Creating Bigrams \n",
    "    top_100_bigrams = bigrams_generator(list(chain(*df['tokens'].tolist())))\n",
    "    mwetokenizer = MWETokenizer(top_100_bigrams)\n",
    "    \n",
    "    df['bigrams'] = df['tokens'].apply(lambda x: mwetokenizer.tokenize(x))\n",
    "    df['bigrams'] = df['bigrams'].apply(lambda x: [token for token in x if '_' in token])\n",
    "    \n",
    "    # Create Unigrams dictionary \n",
    "    uni_grams[key] = sorted(list(dict(FreqDist(list(chain(*df['stem_tokens'].tolist())))).items()),key = lambda x:x[1], reverse = True)[:100]\n",
    "    # Create Bigrams dictionary \n",
    "    bi_grams[key] = sorted(list(dict(FreqDist(list(chain(*df['bigrams'].tolist())))).items()),key = lambda x:x[1],reverse=True)[:100]\n",
    "    \n",
    "    # Joining all the tokens together for generating the vocab text \n",
    "    df['joint_tokens'] = df['stem_tokens'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uni_grams\n",
    "#bi_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we change the structure of our bigrams dictionary \n",
    "# This as per the sample output provided \n",
    "# We are supposed to remove the _ from the bi-grams generated \n",
    "\n",
    "# Creating a new dictionary to store the bigrams as per this format \n",
    "formatted_bigrams = {}\n",
    "\n",
    "# Iterating through the bigrams \n",
    "for key, value in bi_grams.items():\n",
    "    \n",
    "    # Creating a temporary list of words to store the separated bigrams \n",
    "    temp_words_list = []\n",
    "    \n",
    "    # Iterating through the values \n",
    "    for word in value:\n",
    "        \n",
    "        # Remvoving the _ from the bigram words \n",
    "        separate_words = word[0].split(\"_\")\n",
    "        \n",
    "        # Adding the words with the count back to the temporary list\n",
    "        temp_words_list.append(((separate_words[0], separate_words[1]),word[1]))\n",
    "    \n",
    "    # Finally adding the changed formatted list back to the formatted bigrams \n",
    "    formatted_bigrams[key] = temp_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Output File 1 : Unigram and Bigram File\n",
    "\n",
    "* Here, Each line in the txt file contains the top 100 most frequent uni/bigrams of one day of the tweet data.\n",
    "* Format used : sample_100uni.txt and sample_100bi.txt\n",
    "* Naming used : student_number_100uni.txt and student_number_100bi.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the unigrams as per the format provided \n",
    "# <student_number>_100uni.txt \n",
    "with open('30945305_100uni.txt','w+', encoding = 'utf-8') as unigrams_file:\n",
    "    for key, value in uni_grams.items():\n",
    "        unigrams_file.write('%s %s\\n' % (key, value))\n",
    "        \n",
    "# Generating the bigrams as per the format provided  \n",
    "# <student_number>_100bi.txt\n",
    "with open('30945305_100bi.txt','w+', encoding='utf-8') as bigrams_file:\n",
    "    for key, value in formatted_bigrams.items():\n",
    "        bigrams_file.write('%s %s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Corpus Vocabulary and Output File 2 : Vocab Text File\n",
    "\n",
    "* Corpus Vocabulary :\n",
    "    * Here, using CountVectorizer, we generate our vocabulary list.\n",
    "    * We then assign a unique number to each word as per the sample_vocab.txt.\n",
    "* Output File :\n",
    "    * It contains the bigrams and unigrams tokens.\n",
    "    * Format : sample_vocab.txt\n",
    "    * Naming : student_number_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating count vectors \n",
    "# Resource : Study Material Week 5\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") \n",
    "\n",
    "data_features = vectorizer.fit_transform([\" \".join(df['joint_tokens'].tolist()) for key, df in excel_data.items()])\n",
    "#print (data_features.shape)\n",
    "\n",
    "# Creating a vocab dictionary \n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# Creating a dictionary to store all the vocab \n",
    "vocab_dict = dict()\n",
    "\n",
    "# For assigning an unique number to every word \n",
    "# This is for generating the sample vocab text file\n",
    "num = 0\n",
    "for each_word in vocab:\n",
    "    vocab_dict[each_word] = num\n",
    "    num = num + 1\n",
    "    \n",
    "# Generate the corpus vocabulary with the same structure as sample_vocab.txt \n",
    "# <student_number>_vocab.txt\n",
    "with open('30945305_vocab.txt','w+', encoding ='utf-8') as vocab_file:\n",
    "    \n",
    "    for key,value in vocab_dict.items():\n",
    "        \n",
    "        vocab_file.write('%s:%s\\n'%(key,vocab_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Sparse Representation and Output File 3 : Count Vector Text File\n",
    "\n",
    "* Here, we Generate the sparse representation of the excel file \n",
    "* Format : Date, unique_number for the word : Number of times the word occurs in that document \n",
    "* **Methodology followed** :\n",
    "    * We separate out all the dates which are sheet names.\n",
    "    * We create a word count dictionary to store the word count.\n",
    "    * Using the values provided in the data features,\n",
    "        * We zip the vocab list and the word count list.\n",
    "        * This generated a list of tuples according to every index.\n",
    "    * We finally have a dictionary in the following format :\n",
    "    * Key :Date\n",
    "    * Value :Dictionary with { Key : word, Value : Number of times the word occurs in that document }\n",
    "* We then **Clean and Re-format and Re-structure** this dictionary.\n",
    "* Final Format : \n",
    "* Key : Date \n",
    "* Value : {unique_number for the word : Number of times the word occurs in that document}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the sparse representation of the excel file according to sample \n",
    "# Format : Date, unique_number for the word : Number of times the word occurs in that document \n",
    "%%time\n",
    "\n",
    "# We first store our results in a dictionary\n",
    "# Key : Date, # Value : {word : Number of times the word occurs in that document}\n",
    "count_vector = dict()\n",
    "\n",
    "# Changing the count_vector according to our format : formatted version \n",
    "# Key : Date, # Value : {unique_number for the word : Number of times the word occurs in that document}\n",
    "final_count_vector = dict()\n",
    "\n",
    "# For easier iteration\n",
    "array_of_data_features = data_features.toarray()\n",
    "\n",
    "# Getting the dates and adding all the dates to generate the matrix\n",
    "dates = list(excel_data.keys())\n",
    "\n",
    "# In order to get the word count for every word in a given sheet \n",
    "for each_row in array_of_data_features:\n",
    "    \n",
    "    # Dictionary for word count \n",
    "    word_count = dict()\n",
    "    \n",
    "    # Popping out the first element from the list \n",
    "    date = dates.pop(0)\n",
    "    \n",
    "    # Adding the word count to the corresponding word \n",
    "    # Zip : We get a list of tuples corresponding to every index \n",
    "    for word_i, count_i in zip(vocab, each_row):\n",
    "        if count_i > 0:\n",
    "            word_count[word_i] = count_i\n",
    "     \n",
    "    # Finally appending the word_count dictionary corresponding to every date \n",
    "    count_vector[date] = word_count\n",
    "#count_vector\n",
    "\n",
    "# Iterating through our count_vector \n",
    "for key,value in count_vector.items():\n",
    "    \n",
    "    # keys correspond to words \n",
    "    words = value.keys()\n",
    "    \n",
    "    # Dictionary for the final count of occurences of all the words \n",
    "    final_count_dict = dict()\n",
    "    \n",
    "    for each_word in words:\n",
    "        if each_word in vocab_dict:\n",
    "            final_count_dict[vocab_dict[each_word]] = value[each_word]\n",
    "            \n",
    "    final_count_vector[key] = final_count_dict\n",
    "    \n",
    "#final_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the sparse matrix \n",
    "# <student_number>_countVec.txt\n",
    "\n",
    "with open('30945305_countVec.txt','w+') as count_file:\n",
    "    \n",
    "    for key, value in final_count_vector.items():\n",
    "        \n",
    "        # As per the given format, we do not need these characters in the final representation \n",
    "        count_file.write('%s,%s\\n' % (key,str(value).strip('{}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thus, in summary, we have successfully completed the following :\n",
    "    * We have parsed the data sheets and performed the necessary basic data cleaning and tokenisation. \n",
    "    * We have kept only the english tweets.\n",
    "    * We have removed context independent and context dependent words.\n",
    "    * We have further compressed the tokens using Porter Stemmer.\n",
    "    * We have removed words that occur rarely as they have less variance.\n",
    "        * These words do not really give much information to us. \n",
    "    * We have even removed the words which occur way too much!\n",
    "        * These words give redundant information. \n",
    "    * We have generated a sample vocabulary of our data.\n",
    "    * We have even management to get a document frequency of our tokens.\n",
    "    * In addition, we have been able to generate unigrams and bigrams from our words.\n",
    "    * Finally, we have been able to successfully generate our sparse matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
