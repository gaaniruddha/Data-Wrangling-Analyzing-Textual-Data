{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 1 in Assessment 1\n",
    "#### Student Name: Gayatri Aniruddha \n",
    "#### Student ID: 30945305\n",
    "\n",
    "Date: 29/08/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 2.7.11 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* re (for regular expression, included in Anaconda Python 2.7) \n",
    "* langid (to classift the language of a tweet)\n",
    "* os (to interact with the operating system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction \n",
    "\n",
    "* Here, we have been provided with a **twitter dataset text files**.\n",
    "* Our text file contains tweets related to COVID-19.\n",
    "* We have the following information about the tweets :\n",
    "    * id, text and created_at dates.\n",
    "* We need to perform the following tasks on this dataset :\n",
    "    * We need to extract the data present in the text file.\n",
    "    * The extracted data must be transformed into an XML format.\n",
    "* Here, the XML file has id, text and created_at dates.\n",
    "* We have also ensured the following:\n",
    "    * We have filtered out the tweets and have kept only the english tweets.\n",
    "    * We have also ensured that the tweet_id's are unique.\n",
    "* We have also followed this constraint that only 3 packages have been used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "* I have followed the following steps in order to get the desired output for task 1.\n",
    "\n",
    "### 2.1 Importing Libraries\n",
    "* This section contains the code to import the libraries we need in this assessment. \n",
    "* Here, we have imported os, re and langid. \n",
    "* These are the only packages that we are allowed to use for this task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library to interact with the operating system \n",
    "import os \n",
    "\n",
    "# Library for regular expression \n",
    "import re\n",
    "\n",
    "# Library to filter out the non engligh tweets \n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages from langid package\n",
    "# These packages are imported for separating out the english tweets \n",
    "\n",
    "from langid.langid import LanguageIdentifier \n",
    "from langid.langid import model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://github.com/saffsd/langid.py \n",
    "# Initialised the identifier with normal probability True \n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Reading the text files\n",
    "\n",
    "* Here, we have used the os walk function in order to load the data.\n",
    "* We have then created a list to store all the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a list named list_tweet to store the tweets \n",
    "# Here, every element of the list is a tweet \n",
    "list_tweet = []\n",
    "\n",
    "# Here we access the files stored in our system and store the tweets in a list \n",
    "for root, dirs, files in os.walk(\"C:\\\\Users\\\\Gayatri Aniruddha\\\\Desktop\\\\Sem 2 2020\\\\FIT5196 Data Wrangling\\\\Assignment 1\\\\30945305\"):\n",
    "    \n",
    "    # Iterating through every file\n",
    "    for file in files:\n",
    "        # join is the path to the file \n",
    "        with open(os.path.join(root, file), 'r', encoding = 'utf8') as t:\n",
    "            \n",
    "            # Reading the content of the file into tweet\n",
    "            tweet = t.read()\n",
    "            \n",
    "            # Writing the contents of the text into our list \n",
    "            list_tweet.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Reading the tweets.\n",
    "\n",
    "* Here, we have initialised a dictionary to store all the twitter data. \n",
    "* Dummy Dictionary :\n",
    "    * This contains words which have to be changed for data cleaning \n",
    "    * Key is the word that is replaced \n",
    "    * Value is the word that it is replaced with \n",
    "    * Here, we have done this to help eval\n",
    "    * For tweets : we use lower case \n",
    "* Pattern :\n",
    "    * We can always combine a regular expression pattern into pattern objects. \n",
    "    * This is further used for pattern matching. \n",
    "    * This also helps to search a pattern again without having to re-write it again and again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Manipulation and Analysis\n",
    "\n",
    "* We then use the eval function to get our tweets in a dictionary format. \n",
    "* We then take care of the emojis. \n",
    "    * Here, emojis are utf-16 encoded. \n",
    "    * We convert them to utf-8 encoding as utf-8 is the standard format.\n",
    "* Then, using langid, we only keep the tweets in english. \n",
    "* Finally, we add the tweet a dictionary according to the specified format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This contains words which have to be changed for data cleaning \n",
    "# Key is the word that is replaced \n",
    "# Value is the word that it is replaced with \n",
    "# This has been done to facilitate the eval function\n",
    "dummy_dictionary = {'true': 'True', 'false': 'False'}\n",
    "\n",
    "# Here we tell the compiler that it is just a variable. \n",
    "# Do not worry about it and go ahead with the reading \n",
    "# Here, we are saving the regex in pattern \n",
    "pattern = re.compile(r\"(true|false)\")\n",
    "\n",
    "# Initializing a dictionary data to store the wrangled data \n",
    "tweet_data = dict()\n",
    "\n",
    "# Here, we iterate through each item in the created list\n",
    "# Where, each item is a tweet \n",
    "for each in list_tweet:\n",
    "    \n",
    "    # Replace our defined words for the text \n",
    "    # Replacing true with True and false False\n",
    "    # Using the dummy_dictionary and pattern function\n",
    "    # This is for finding this particular pattern and substitution \n",
    "    tweet_words = pattern.sub(lambda x: dummy_dictionary.get(x.group(1), x.group(1)), each)\n",
    "    \n",
    "    # Using eval to get our data into dictionary format\n",
    "    twitter_eval_dict = eval(tweet_words)\n",
    "    \n",
    "    # Iterate through our evaluated dictionary items \n",
    "    for key, value in twitter_eval_dict.items():\n",
    "        \n",
    "        # This is as per the specification provided \n",
    "        if key == 'data':\n",
    "            \n",
    "            # Here, we go through the every item in the list of values \n",
    "            # Where, every value has the tweet id and the tweet \n",
    "            for each_item in value:\n",
    "                 \n",
    "                # emojis follow utf 16 encoding\n",
    "                # Here, we handle the utf-16 encoded emojis and bring them to a stand utf-8 format\n",
    "                # utf 8 encoding is standard format\n",
    "                encoded_item = each_item['text'].encode('utf-16','surrogatepass').decode('utf-16')\n",
    "                \n",
    "                # Here, we ensure that only the tweets in english are present in the final dictionary\n",
    "                if identifier.classify(encoded_item)[0] == 'en':\n",
    "                    \n",
    "                    # Finally, we append the tweet with the created_at date in the format specified \n",
    "                    tweet_data[each_item['id']] = [encoded_item,re.search(r'\\d{4}-\\d{2}-\\d{2}',each_item['created_at']).group()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Extraction\n",
    "\n",
    "* We now generate a dictionary to sort the tweets according to the dates.\n",
    "* We then store the wrangled data into an xml file as per the specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a dictionary date_sorted to store tweets according to dates \n",
    "date_sorted = {}\n",
    "\n",
    "# Here, we then change the structure of our wrangled tweet_data\n",
    "# Here, we group all the tweets according to their created_at date\n",
    "\n",
    "# Here, we use the sorted function to sort a dictionary by it's value \n",
    "for key, value in sorted(tweet_data.items()):\n",
    "    date_sorted.setdefault(value[1], []).append([key,value[0]])\n",
    "\n",
    "# Final Step : \n",
    "# Here we are storing and submitting the extracted data into an XML file\n",
    "# The XML File has been named as per the specification provided \n",
    "\n",
    "# open: In order to create a new file for writing \n",
    "# w+ : opens the file for reading and writing \n",
    "# encoding : utf-8 \n",
    "with open('30945305.xml','w+', encoding='utf-8') as my_xml_file:\n",
    "    \n",
    "    # Adding this data according to the sample xml file \n",
    "    # write is used \n",
    "    my_xml_file.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    \n",
    "    # We are filling our XML File as per the sample file\n",
    "    my_xml_file.write('<data>\\n')\n",
    "    \n",
    "    # Iterating through the date_sorted dictionary \n",
    "    for key, values in date_sorted.items():\n",
    "        \n",
    "        my_xml_file.write('<tweets date=\"%s\">\\n' % (key))\n",
    "        \n",
    "        for every_item in values:\n",
    "            my_xml_file.write('<tweet id=\"%s\">%s</tweet>\\n' % (every_item[0],every_item[1]))\n",
    "            \n",
    "        my_xml_file.write('</tweets>\\n')\n",
    "        \n",
    "    my_xml_file.write('</data>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n",
    "* Thus, in summary we have successfully performed the following tasks :\n",
    "    * We have successfully extracted the data in the desired format.\n",
    "    * We have managed to properly use the langid package.\n",
    "    * We have ensured the uniqueness of the tweets.\n",
    "    * We have also managed to create a resultant XML file in the required format. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
